I'll help you analyze CPU limits and configure HPA properly for your multi-AZ setup. Here's a systematic approach:

1. Current State Analysis

Check Current Resource Usage

bash
# Get current CPU usage across pods
kubectl top pods --all-namespaces --sort-by=cpu

# Get node resource allocation
kubectl top nodes

# Check current resource requests/limits
kubectl describe nodes | grep -A 10 "Allocated resources"

# Check pod resource configuration
kubectl get pods -o wide
kubectl describe pod <pod-name> | grep -A 5 "Limits"
Analyze CPU Limits vs Usage

bash
# Get detailed metrics for HPA analysis
kubectl get hpa --all-namespaces
kubectl describe hpa <hpa-name>
2. Identify Bottlenecks

Check Which Services are Hitting Limits

bash
# Monitor real-time CPU usage
kubectl top pods --containers

# Check for throttled containers
kubectl get events --sort-by=.lastTimestamp | grep -i "cpu"
Analyze Application Patterns

bash
# Check if limits are consistently hit or just during peaks
kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods | jq '.items[] | {pod: .metadata.name, cpu: .containers[].usage.cpu}'
3. HPA Configuration Strategy

Service-Specific HPA Configuration

For CPU-intensive services:

yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: cpu-intensive-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cpu-intensive-service
  minReplicas: 3  # One per AZ
  maxReplicas: 15
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Conservative target for CPU-heavy apps
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
For Memory-intensive services:

yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: memory-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: memory-service
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
For Mixed workloads:

yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mixed-workload-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mixed-service
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 65
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 75
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 600
    scaleUp:
      stabilizationWindowSeconds: 60
4. Pod Resource Optimization

Update Resource Requests/Limits

yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: optimized-service
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: your-app:latest
        resources:
          requests:
            cpu: "250m"    # Start conservative
            memory: "512Mi"
          limits:
            cpu: "1000m"   # Based on analysis
            memory: "1Gi"
5. Multi-AZ Considerations

Pod Anti-Affinity for AZ Distribution

yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: az-aware-deployment
spec:
  replicas: 3
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - your-app
              topologyKey: topology.kubernetes.io/zone
6. Monitoring and Validation

Create Comprehensive Monitoring

bash
# Set up ongoing monitoring
kubectl get hpa -w

# Check HPA events
kubectl describe hpa <hpa-name>

# Monitor scaling events
kubectl get events --field-selector reason=SuccessfulRescale
HPA Status Check Script

bash
#!/bin/bash
echo "=== HPA Status ==="
kubectl get hpa --all-namespaces

echo -e "\n=== Current vs Desired Replicas ==="
kubectl get hpa -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.currentReplicas}{"/"}{.status.desiredReplicas}{"\n"}{end}'

echo -e "\n=== CPU Utilization ==="
kubectl get hpa -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.currentCPUUtilizationPercentage}{"%\n"}{end}'
7. Step-by-Step Implementation Plan

Week 1: Analysis

Run monitoring commands to identify patterns
Document which services hit CPU limits most frequently
Note peak usage times and patterns
Week 2: Conservative HPA

Start with conservative targets (70-80% CPU)
Implement for 2-3 most critical services
Monitor scaling behavior
Week 3: Optimize

Adjust based on observed patterns
Fine-tune stabilization windows
Implement for remaining services
Week 4: Review

Analyze cost/performance impact
Adjust min/max replicas as needed
Document configurations
8. Troubleshooting Common Issues

bash
# If HPA isn't scaling
kubectl describe hpa <name>  # Check events
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes  # Check metrics API

# If scaling is too aggressive
kubectl patch hpa <name> -p '{"spec":{"behavior":{"scaleUp":{"stabilizationWindowSeconds":180}}}}'

# Check resource quotas
kubectl describe resourcequota
Start with the analysis phase to understand your specific patterns, then implement HPA gradually while monitoring the impact. The multi-AZ setup requires careful consideration of minimum replicas to maintain availability across zones.

