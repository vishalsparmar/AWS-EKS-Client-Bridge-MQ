. Root Cause

When eu-west-2a fails, CoreDNS pods in that AZ become unreachable, but Kubernetes service endpoints might still point to them.

2. Immediate Fix - Check CoreDNS Distribution

bash
# Check where CoreDNS pods are running
kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide

# Check which AZs have CoreDNS pods
kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide | awk '{print $7}' | xargs -I {} kubectl get node {} -o jsonpath='{.metadata.labels.topology\.kubernetes\.io/zone}' && echo
3. Force CoreDNS to Multiple AZs

Add Pod Anti-Affinity:

yaml
# Patch CoreDNS deployment for multi-AZ spread
kubectl patch deployment coredns -n kube-system -p '{
  "spec": {
    "template": {
      "spec": {
        "affinity": {
          "podAntiAffinity": {
            "requiredDuringSchedulingIgnoredDuringExecution": [
              {
                "labelSelector": {
                  "matchExpressions": [
                    {
                      "key": "k8s-app",
                      "operator": "In",
                      "values": ["kube-dns"]
                    }
                  ]
                },
                "topologyKey": "topology.kubernetes.io/zone"
              }
            ]
          }
        }
      }
    }
  }
}'
Scale CoreDNS:

bash
# Scale CoreDNS to have pods in each AZ
kubectl scale deployment coredns -n kube-system --replicas=6
4. Check Service Endpoints

bash
# Check kube-dns service endpoints
kubectl get endpoints kube-dns -n kube-system

# Verify endpoints are spread across AZs
kubectl get endpoints kube-dns -n kube-system -o yaml | grep ip
5. NodeLocal DNS Cache (Recommended)

Set up NodeLocal DNS cache to prevent AZ dependency:

yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nodelocaldns
  namespace: kube-system
  labels:
    k8s-app: nodelocaldns
spec:
  selector:
    matchLabels:
      k8s-app: nodelocaldns
  template:
    metadata:
      labels:
        k8s-app: nodelocaldns
    spec:
      priorityClassName: system-node-critical
      hostNetwork: true
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists
      containers:
      - name: node-cache
        image: registry.k8s.io/dns/k8s-dns-node-cache:1.22.20
        resources:
          requests:
            cpu: 25m
            memory: 5Mi
        args:
        - -localip
        - 169.254.20.10
        - -conf
        - /etc/coredns/Corefile
        - -upstreamsvc
        - kube-dns
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        livenessProbe:
          httpGet:
            host: 169.254.20.10
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
6. Temporary Workaround During Outage

bash
# Manually delete CoreDNS pods in failed AZ
kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide | grep eu-west-2a | awk '{print $1}' | xargs kubectl delete pod -n kube-system

# Or cordon the problematic AZ
kubectl get nodes -l topology.kubernetes.io/zone=eu-west-2a -o name | xargs kubectl cordon
7. Preventative Configuration

Update CoreDNS ConfigMap:

yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance   # Add this for DNS load balancing
    }
8. Verify Fix

bash
# After changes, verify CoreDNS distribution
kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide | \
  awk 'NR>1{print $7}' | \
  xargs -I {} kubectl get node {} -o jsonpath='{.metadata.labels.topology\.kubernetes\.io/zone}' | \
  sort | uniq -c

# Test DNS resolution during AZ failure simulation
kubectl run dns-test --image=busybox --rm -it --restart=Never -- \
  sh -c "while true; do nslookup kubernetes.default.svc.cluster.local; sleep 5; done"
9. AWS-Specific DNS Settings

bash
# Check if using Amazon DNS server in VPC
aws ec2 describe-vpcs --vpc-ids <your-vpc> --query 'Vpcs[0].DhcpOptionsId'

# Ensure VPC DNS settings are correct
# Should use AmazonProvidedDNS or your own resolvers in each AZ
The key is ensuring CoreDNS pods are distributed across ALL AZs and implementing NodeLocal DNS cache to handle AZ failures gracefully.

