Create a Dedicated Node Group

bash
eksctl create nodegroup \
  --cluster my-cluster \
  --name dedicated-nodes \
  --node-type t3.medium \
  --nodes 1 \
  --nodes-min 1 \
  --nodes-max 3 \
  --taints dedicated=microservice-a:NoSchedule
Apply Taints and Tolerations

yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: microservice-a
spec:
  replicas: 2
  selector:
    matchLabels:
      app: microservice-a
  template:
    metadata:
      labels:
        app: microservice-a
    spec:
      tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "microservice-a"
        effect: "NoSchedule"
      containers:
      - name: microservice-a
        image: my-image:latest
2. Using Node Affinity + Pod Anti-Affinity

Combined Approach for Maximum Isolation

yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: microservice-a
spec:
  replicas: 2
  selector:
    matchLabels:
      app: microservice-a
  template:
    metadata:
      labels:
        app: microservice-a
    spec:
      affinity:
        # Node affinity - only run on dedicated nodes
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: dedicated
                operator: In
                values:
                - microservice-a
        
        # Pod anti-affinity - don't schedule with other pods
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: Exists
            topologyKey: kubernetes.io/hostname
      containers:
      - name: microservice-a
        image: my-image:latest
3. Complete Example with Terraform

Node Group Configuration

hcl
resource "aws_eks_node_group" "dedicated" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "microservice-a-dedicated"
  node_role_arn   = aws_iam_role.node_group.arn
  subnet_ids      = aws_subnet.private[*].id

  capacity_type  = "ON_DEMAND"
  instance_types = ["t3.medium"]

  scaling_config {
    desired_size = 2
    max_size     = 4
    min_size     = 1
  }

  # Add taint to the node group
  taint {
    key    = "dedicated"
    value  = "microservice-a"
    effect = "NO_SCHEDULE"
  }

  labels = {
    dedicated = "microservice-a"
  }

  depends_on = [
    aws_iam_role_policy_attachment.node_group,
  ]
}
Kubernetes Deployment

yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: microservice-a
  labels:
    app: microservice-a
spec:
  replicas: 2
  selector:
    matchLabels:
      app: microservice-a
  template:
    metadata:
      labels:
        app: microservice-a
    spec:
      tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "microservice-a"
        effect: "NoSchedule"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: dedicated
                operator: In
                values:
                - microservice-a
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - microservice-a
            topologyKey: kubernetes.io/hostname
      containers:
      - name: microservice-a
        image: nginx:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
4. Using Kubernetes Namespaces and Resource Quotas

Namespace with Node Selector

yaml
apiVersion: v1
kind: Namespace
metadata:
  name: microservice-a-ns
  labels:
    name: microservice-a-ns
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: namespace-admission
  namespace: microservice-a-ns
data:
  pod-node-selector: "dedicated=microservice-a"
5. Verification Commands

Check Node Taints and Labels

bash
kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints,LABELS:.metadata.labels

# Check which pods are running on which nodes
kubectl get pods -o wide

# Describe a specific node to see taints and labels
kubectl describe node <node-name>
Verify Pod Distribution

bash
# Check if pods are isolated as expected
kubectl get pods -o wide --selector=app=microservice-a

# Verify no other pods on dedicated nodes
kubectl describe node <dedicated-node-name> | grep -A 10 "Non-terminated Pods"
6. Best Practices

Use both taints and affinity for defense-in-depth
Set resource requests/limits to ensure proper node sizing
Monitor node utilization to right-size your dedicated nodes
Use horizontal pod autoscaling with cluster autoscaling
Consider using spot instances for cost savings if appropriate
7. Cleanup

bash
# Delete the deployment
kubectl delete deployment microservice-a

# Delete the node group
eksctl delete nodegroup --cluster my-cluster --name dedicated-nodes
This approach ensures your microservice runs exclusively on dedicated nodes while maintaining high availability and proper resource isolation.
spa and deployment replicas hoot sync so I want min 15 and max 20
For syncing SPA deployments with replicas (min 15, max 20), you have several approaches depending on your architecture. Here are the most effective solutions:

1. Horizontal Pod Autoscaler (HPA) with Custom Metrics

Basic H
Hpa and deployment replicas hoot sync so I want min 15 and max 20
Here are several approaches to sync HPA with deployment replicas maintaining min 15 and max 20:

1. Basic HPA Configuration

yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 15
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 20
        periodSeconds: 60
      - type: Pods
        value: 5
        periodSeconds: 60
      selectPolicy: Max
2. Multi-Metric HPA for Better Sync

yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 15
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 65
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: 100
3. Deployment with Resource Limits

yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  labels:
    app: my-app
spec:
  replicas: 15  # Initial count matching HPA min
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
        image: my-app:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 20
4. Custom Metrics with Prometheus

yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-custom-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 15
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: External
    external:
      metric:
        name: http_requests_per_second
        selector:
          matchLabels:
            service: my-app
      target:
        type: AverageValue
        averageValue: "100"
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        name: my-app-ingress
      target:
        type: Value
        value: 1000
5. HPA with Custom Behavior for Smooth Scaling

yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-stable-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 15
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  behavior:
    # Scale down slowly, scale up quickly
    scaleDown:
      stabilizationWindowSeconds: 600  # 10 minutes
      policies:
      - type: Percent
        value: 10
        periodSeconds: 120
      - type: Pods
        value: 1
        periodSeconds: 120
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60   # 1 minute
      policies:
      - type: Percent
        value: 30
        periodSeconds: 60
      - type: Pods
        value: 5
        periodSeconds: 60
      selectPolicy: Max
6. Verification and Monitoring Commands

Check HPA Status

bash
# Get HPA status
kubectl get hpa my-app-hpa -w

# Describe HPA for detailed info
kubectl describe hpa my-app-hpa

# Check current replicas
kubectl get deployment my-app

# Monitor pod distribution
kubectl get pods -l app=my-app
HPA Status Output Example

text
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
my-app-hpa   Deployment/my-app      75%/70%   15        20        18         5d
7. Kubernetes Events Monitoring

bash
# Watch HPA events
kubectl get events --field-selector involvedObject.kind=HorizontalPodAutoscaler --sort-by=.lastTimestamp

# Check HPA metrics
kubectl top pods -l app=my-app
8. Best Practices for Min 15 - Max 20 Setup

Proper Resource Requests: Ensure CPU/memory requests are set correctly
Cluster Capacity: Make sure your cluster can handle 20 replicas
Node Distribution: Use pod anti-affinity to spread across nodes
Monitoring: Set up alerts when approaching max replicas
Load Testing: Test scaling behavior under load
9. Cluster Autoscaler Integration

If using cluster autoscaler, ensure node groups can accommodate:

yaml
# Example node group sizing for 20 pods
resources:
  requests:
    cpu: 200m * 20 = 4 CPU
    memory: 256Mi * 20 = 5.12 GB
This setup ensures your deployment maintains at least 15 replicas, scales up to 20 based on load, and provides smooth, predictable scaling behavior.


